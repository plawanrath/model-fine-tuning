# How to run
# export PYTORCH_ENABLE_MPS_FALLBACK=1
# export OMP_NUM_THREADS=8

# Train
# python -m axolotl.cli.train mistral_lora_offline.yaml


# merge
# python -m axolotl.cli.merge_lora mistral_lora_offline.yaml

# ── 1. model & data ────────────────────────────────────────────────────────
base_model: ../models/Mistral-7B-v0.1
trust_remote_code: true

use_peft: true
peft_type: lora

output_dir: ../output/mistral-lora

datasets:
  - path: ../../../rust_alpaca.json
    type: alpaca

tokenizer:
  truncation_side: left

lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: [q_proj, k_proj, v_proj, o_proj]

micro_batch_size: 4
gradient_accumulation_steps: 4

num_epochs: 3
learning_rate: 1.5e-4
lr_scheduler_type: cosine
warmup_steps: 10

save_strategy: "epoch"
logging_steps: 10

# Required for Apple Silicon
precision: float32
fp16: false
bf16: false
mixed_precision: "no"
gradient_checkpointing: true

load_in_8bit: false
load_in_4bit: false
